{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0dc17e-82be-45c2-b648-f607fe487bfd",
   "metadata": {},
   "source": [
    "# Parsing and Annotating Data\n",
    "\n",
    "Parsing the raw data into the three core tables of your addition: the LIB, CORPUS, and VOCAB tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7fb1d6-cd0f-4934-ae68-4416bd30207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f2aa26-fbbe-410d-8855-98ca4a3ec552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac705d18-2c5f-4750-b54f-f65ab3a7381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing parser module\n",
    "from textparser import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e65ca0a-fa86-4d87-989e-30382b9c5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "source_files = \"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8\"\n",
    "\n",
    "# define OHCO\n",
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc22a3b-4cf4-40e9-992c-ecaec9eab033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing boiler plates\n",
    "clip_pats = [\n",
    "    r\"(?m)^THE START\\s*$\",\n",
    "    r\"(?m)^THE END\\s*$\"\n",
    "]\n",
    "\n",
    "# chunk by chapter\n",
    "\n",
    "ohco_pat_list = [\n",
    "    ('BetweenTheActs', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('Flush', r'^(CHAPTER\\s+[A-Z]+)\\s*$'), # CHAPTER X (blank line) chapter name\n",
    "    ('JacobsRoom', r'^CHAPTER\\s+[A-Z]+\\s*$'), # CHAPTER X\n",
    "    ('MrsDalloway', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('NightAndDay', r'^CHAPTER\\s+[IVXLCDM]+\\s*$'),# CHAPTER ? (roman numeral)\n",
    "    ('Orlando', r'^CHAPTER\\s+\\d+\\.\\s*$'), # CHAPTER X. \n",
    "    ('TheVoyageOut', r'^Chapter\\s+[IVXLCDM]+\\s*$'), # Chapter ? (roman numeral)\n",
    "    ('TheWaves', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('TheYears', r'^\\s*(18|19)\\d{2}\\s*$'), # blank line, year, blank line\n",
    "    ('ToTheLighthouse', r'^\\s*\\d+\\s*$'), # blank line, number, blank line\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324e2dcd-0a44-4b9b-9695-a56386fdec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register each file to a library\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    book_id = source_file_path.split('/')[-1].replace('.utf8.txt', '')\n",
    "    book_title = source_file_path.split('/')[-1].replace('.utf8.txt', '')\n",
    "    book_data.append((book_id, source_file_path, book_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd82197-3d25-48ea-82bc-05a892244779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.DataFrame(book_data, columns=['book_id','source_file_path','title'])\\\n",
    "    .set_index('book_id').sort_index()\n",
    "\n",
    "# add chapter regexes\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "\n",
    "# add publication year\n",
    "publication_years = {\n",
    "    'TheVoyageOut': 1915,\n",
    "    'NightAndDay': 1919,\n",
    "    'JacobsRoom': 1922,\n",
    "    'MrsDalloway': 1925,\n",
    "    'ToTheLighthouse': 1927,\n",
    "    'Orlando': 1928,\n",
    "    'TheWaves': 1931,\n",
    "    'Flush': 1933,\n",
    "    'TheYears': 1937,\n",
    "    'BetweenTheActs': 1941\n",
    "}\n",
    "\n",
    "LIB['year'] = LIB['title'].map(publication_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e314df-934e-4e6d-9cb2-2a8390d81482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BetweenTheActs</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>BetweenTheActs</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flush</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>Flush</td>\n",
       "      <td>^(CHAPTER\\s+[A-Z]+)\\s*$</td>\n",
       "      <td>1933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JacobsRoom</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>JacobsRoom</td>\n",
       "      <td>^CHAPTER\\s+[A-Z]+\\s*$</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrsDalloway</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>MrsDalloway</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightAndDay</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>NightAndDay</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCDM]+\\s*$</td>\n",
       "      <td>1919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orlando</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>^CHAPTER\\s+\\d+\\.\\s*$</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheVoyageOut</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheVoyageOut</td>\n",
       "      <td>^Chapter\\s+[IVXLCDM]+\\s*$</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheWaves</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheWaves</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheYears</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheYears</td>\n",
       "      <td>^\\s*(18|19)\\d{2}\\s*$</td>\n",
       "      <td>1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ToTheLighthouse</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>ToTheLighthouse</td>\n",
       "      <td>^\\s*\\d+\\s*$</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source_file_path  \\\n",
       "book_id                                                              \n",
       "BetweenTheActs   /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "Flush            /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "JacobsRoom       /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "MrsDalloway      /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "NightAndDay      /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "Orlando          /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheVoyageOut     /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheWaves         /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheYears         /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "ToTheLighthouse  /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "\n",
       "                           title                 chap_regex  year  \n",
       "book_id                                                            \n",
       "BetweenTheActs    BetweenTheActs            ^###CHAPTER###$  1941  \n",
       "Flush                      Flush    ^(CHAPTER\\s+[A-Z]+)\\s*$  1933  \n",
       "JacobsRoom            JacobsRoom      ^CHAPTER\\s+[A-Z]+\\s*$  1922  \n",
       "MrsDalloway          MrsDalloway            ^###CHAPTER###$  1925  \n",
       "NightAndDay          NightAndDay  ^CHAPTER\\s+[IVXLCDM]+\\s*$  1919  \n",
       "Orlando                  Orlando       ^CHAPTER\\s+\\d+\\.\\s*$  1928  \n",
       "TheVoyageOut        TheVoyageOut  ^Chapter\\s+[IVXLCDM]+\\s*$  1915  \n",
       "TheWaves                TheWaves            ^###CHAPTER###$  1931  \n",
       "TheYears                TheYears       ^\\s*(18|19)\\d{2}\\s*$  1937  \n",
       "ToTheLighthouse  ToTheLighthouse                ^\\s*\\d+\\s*$  1927  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12e00b85-f37b-4608-84cf-0bb70f053eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "LIB.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/lib.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259424a3-4a57-4904-911f-d3288f3dea2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'end'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book_id \u001b[38;5;129;01min\u001b[39;00m target_books:\n\u001b[1;32m     31\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m LIB\u001b[38;5;241m.\u001b[39mloc[book_id]\u001b[38;5;241m.\u001b[39msource_file_path\n\u001b[0;32m---> 32\u001b[0m     \u001b[43minsert_chapter_markers_exact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     LIB\u001b[38;5;241m.\u001b[39mat[book_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchap_regex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^###CHAPTER###$\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36minsert_chapter_markers_exact\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# split text into two parts: before and after \"THE START\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m start_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?m)^THE START\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*$\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m----> 9\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[43mstart_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m()\n\u001b[1;32m     10\u001b[0m header \u001b[38;5;241m=\u001b[39m text[:start_idx]\n\u001b[1;32m     11\u001b[0m body \u001b[38;5;241m=\u001b[39m text[start_idx:]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'end'"
     ]
    }
   ],
   "source": [
    "# creating chapter markers for books with sections divided by multiple blank lines\n",
    "def insert_chapter_markers_exact(file_path):\n",
    "    # read in books\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # split text into two parts: before and after \"THE START\"\n",
    "    start_match = re.search(r'(?m)^THE START\\s*$', text)\n",
    "    \n",
    "    start_idx = start_match.end()\n",
    "    header = text[:start_idx]\n",
    "    body = text[start_idx:]\n",
    "\n",
    "    # insert chapter marker immediately after \"THE START\"\n",
    "    body = re.sub(r'^(\\s*)', r'###CHAPTER###\\n\\1', body, count=1)\n",
    "\n",
    "    # replace exactly 5 blank lines with chapter marker\n",
    "    five_blank_pattern = r'(?m)(?:^[ \\t]*\\r?\\n){5}(?=^[^\\s])'\n",
    "    body = re.sub(five_blank_pattern, '\\n###CHAPTER###\\n', body)\n",
    "\n",
    "    # write back to file\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(header + body)\n",
    "\n",
    "    # confirm with print statement\n",
    "    print(f\"✅ Inserted chapter markers after 'THE START' and 5 blank lines in {file_path}\")\n",
    "\n",
    "\n",
    "# apply to the books that need it\n",
    "target_books = ['BetweenTheActs', 'MrsDalloway', 'TheWaves']\n",
    "for book_id in target_books:\n",
    "    file_path = LIB.loc[book_id].source_file_path\n",
    "    insert_chapter_markers_exact(file_path)\n",
    "    LIB.at[book_id, 'chap_regex'] = r'^###CHAPTER###$'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e281b-def6-42fa-a122-52e696286787",
   "metadata": {},
   "source": [
    "^ returns error if ran already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7316674-016a-4fbc-b816-677c7fd691fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing function\n",
    "def tokenize_collection(LIB):\n",
    "    clip_pats = [\n",
    "    r\"(?m)^THE START\\s*$\",\n",
    "    r\"(?m)^THE END\\s*$\"\n",
    "    ]\n",
    "    \n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "        try:\n",
    "            print(f\"Tokenizing {book_id} {LIB.loc[book_id].title}\")\n",
    "            \n",
    "            chap_regex = LIB.loc[book_id].chap_regex\n",
    "            ohco_pats = [('chap', chap_regex, 'm')]\n",
    "            src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "            text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "            text.verbose = True\n",
    "            text.strip_hyphens = True\n",
    "            text.strip_whitespace = True\n",
    "\n",
    "            # debug: check if chapter regex is matching anything\n",
    "            with open(src_file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            matching_lines = pd.DataFrame({'line': [line.strip() for line in lines]})\n",
    "            num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
    "            print(f\"Found {num_matches} matching chapter headings for {book_id}\")\n",
    "\n",
    "            text.import_source().parse_tokens()\n",
    "            text.TOKENS['book_id'] = book_id\n",
    "            text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "            books.append(text.TOKENS)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Failed on {book_id}: {LIB.loc[book_id].title}\")\n",
    "            print(f\"Error: {e}\\n\")\n",
    "    \n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "    print(\"✅ Done\")\n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310bc25a-d607-426c-8afb-254424977bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing BetweenTheActs BetweenTheActs\n",
      "Found 36 matching chapter headings for BetweenTheActs\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/BetweenTheActs.utf8.txt\n",
      "Clipping text\n",
      "\n",
      " Failed on BetweenTheActs: BetweenTheActs\n",
      "Error: Clip start pattern not found.\n",
      "\n",
      "Tokenizing Flush Flush\n",
      "Found 6 matching chapter headings for Flush\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/Flush.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(CHAPTER\\s+[A-Z]+)\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/7jsc8p0d2kdg9phz71qccnd00000gn/T/ipykernel_33756/1828944259.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
      "/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis/textparser.py:97: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing JacobsRoom JacobsRoom\n",
      "Found 14 matching chapter headings for JacobsRoom\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/JacobsRoom.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[A-Z]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing MrsDalloway MrsDalloway\n",
      "Found 10 matching chapter headings for MrsDalloway\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/MrsDalloway.utf8.txt\n",
      "Clipping text\n",
      "\n",
      " Failed on MrsDalloway: MrsDalloway\n",
      "Error: Clip start pattern not found.\n",
      "\n",
      "Tokenizing NightAndDay NightAndDay\n",
      "Found 34 matching chapter headings for NightAndDay\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/NightAndDay.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[IVXLCDM]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing Orlando Orlando\n",
      "Found 6 matching chapter headings for Orlando\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/Orlando.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+\\d+\\.\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing TheVoyageOut TheVoyageOut\n",
      "Found 27 matching chapter headings for TheVoyageOut\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheVoyageOut.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^Chapter\\s+[IVXLCDM]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing TheWaves TheWaves\n",
      "Found 29 matching chapter headings for TheWaves\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheWaves.utf8.txt\n",
      "Clipping text\n",
      "\n",
      " Failed on TheWaves: TheWaves\n",
      "Error: Clip start pattern not found.\n",
      "\n",
      "Tokenizing TheYears TheYears\n",
      "Found 10 matching chapter headings for TheYears\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheYears.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*(18|19)\\d{2}\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/7jsc8p0d2kdg9phz71qccnd00000gn/T/ipykernel_33756/1828944259.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
      "/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis/textparser.py:97: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing ToTheLighthouse ToTheLighthouse\n",
      "Found 43 matching chapter headings for ToTheLighthouse\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/ToTheLighthouse.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "✅ Done\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531a70bd-5bc0-41b2-99d9-bb1b0f62ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Flush</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(Three, CD)</td>\n",
       "      <td>CD</td>\n",
       "      <td>Three</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Mile, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Mile</td>\n",
       "      <td>mile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Cross, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Cross</td>\n",
       "      <td>cross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(It, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>It</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(is, VBZ)</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ToTheLighthouse</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">43</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">9</th>\n",
       "      <th>10</th>\n",
       "      <td>(I, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(have, VBP)</td>\n",
       "      <td>VBP</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(had, VBN)</td>\n",
       "      <td>VBN</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(my, PRP$)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(vision., NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>vision.</td>\n",
       "      <td>vision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675852 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         pos_tuple   pos  \\\n",
       "book_id         chap_id para_num sent_num token_num                        \n",
       "Flush           1       0        0        0            (Three, CD)    CD   \n",
       "                                          1            (Mile, NNP)   NNP   \n",
       "                                          2           (Cross, NNP)   NNP   \n",
       "                        1        0        0              (It, PRP)   PRP   \n",
       "                                          1              (is, VBZ)   VBZ   \n",
       "...                                                            ...   ...   \n",
       "ToTheLighthouse 43      3        9        10              (I, PRP)   PRP   \n",
       "                                          11           (have, VBP)   VBP   \n",
       "                                          12            (had, VBN)   VBN   \n",
       "                                          13            (my, PRP$)  PRP$   \n",
       "                                          14         (vision., NN)    NN   \n",
       "\n",
       "                                                    token_str term_str  \n",
       "book_id         chap_id para_num sent_num token_num                     \n",
       "Flush           1       0        0        0             Three    three  \n",
       "                                          1              Mile     mile  \n",
       "                                          2             Cross    cross  \n",
       "                        1        0        0                It       it  \n",
       "                                          1                is       is  \n",
       "...                                                       ...      ...  \n",
       "ToTheLighthouse 43      3        9        10                I        i  \n",
       "                                          11             have     have  \n",
       "                                          12              had      had  \n",
       "                                          13               my       my  \n",
       "                                          14          vision.   vision  \n",
       "\n",
       "[675852 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f1debe-4f13-407d-aecb-0348f160d2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id\n",
       "Flush               34610\n",
       "JacobsRoom          55494\n",
       "NightAndDay        168036\n",
       "Orlando             79225\n",
       "TheVoyageOut       137843\n",
       "TheYears           130731\n",
       "ToTheLighthouse     69913\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.groupby('book_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e14908-003e-41b5-a495-984e3e6f89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "CORPUS.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b7941b0-7ea8-4dd8-b77d-1324e1f0f8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token_str\n",
       "\"        439\n",
       ".\"       215\n",
       "\".        81\n",
       "...\"      34\n",
       "?\"        31\n",
       "....       8\n",
       "),         4\n",
       "'          4\n",
       ".'         3\n",
       "'\"         3\n",
       "&          3\n",
       "!\"         3\n",
       "\";         3\n",
       "\"...       3\n",
       "***        2\n",
       ",\"         2\n",
       ".'\"        2\n",
       "\"'         2\n",
       "\"'.        2\n",
       ",'         1\n",
       "...?\"      1\n",
       ".)         1\n",
       "?'\"        1\n",
       "?)         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating VOCAB table\n",
    "\n",
    "# handling anomalies\n",
    "CORPUS[CORPUS.term_str == ''].token_str.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95e5fbb6-c299-4d0b-a180-d2568e3e4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty term_str (punctuation)\n",
    "CORPUS = CORPUS[CORPUS.term_str != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f96f14-2817-4f70-b4c3-1fbde93d7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocab table\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "\n",
    "# getting max POS (most frequently associated part-of-speech for each word)\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e3291f-4b53-4a42-859e-70e6276bb37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dispensed</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rejoiced</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sped</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feels</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inky</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insect</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happening</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industries</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blaring</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>process</th>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n  n_chars max_pos\n",
       "term_str                       \n",
       "dispensed    2        9     VBN\n",
       "rejoiced     1        8      NN\n",
       "sped         3        4     VBD\n",
       "feels       23        5     VBZ\n",
       "inky         2        4      VB\n",
       "insect      14        6      NN\n",
       "happening   23        9     VBG\n",
       "industries   2       10     NNS\n",
       "blaring      1        7      NN\n",
       "process     38        7      NN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80379478-dc22-4a7f-b180-17a0bff796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "VOCAB.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3690a-a226-4cb8-bc74-588b5dfa1274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
