{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0dc17e-82be-45c2-b648-f607fe487bfd",
   "metadata": {},
   "source": [
    "# Parsing and Annotating Data\n",
    "\n",
    "Parsing the raw data into the three core tables of your addition: the LIB, CORPUS, and VOCAB tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7fb1d6-cd0f-4934-ae68-4416bd30207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f2aa26-fbbe-410d-8855-98ca4a3ec552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac705d18-2c5f-4750-b54f-f65ab3a7381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing parser module\n",
    "from textparser import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e65ca0a-fa86-4d87-989e-30382b9c5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "source_files = \"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8\"\n",
    "\n",
    "# define OHCO\n",
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc22a3b-4cf4-40e9-992c-ecaec9eab033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing boiler plates\n",
    "clip_pats = [\n",
    "    r\"(?m)^THE START\\s*$\",\n",
    "    r\"(?m)^THE END\\s*$\"\n",
    "]\n",
    "\n",
    "# chunk by chapter\n",
    "\n",
    "ohco_pat_list = [\n",
    "    ('BetweenTheActs', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('Flush', r'^(CHAPTER\\s+[A-Z]+)\\s*$'), # CHAPTER X (blank line) chapter name\n",
    "    ('JacobsRoom', r'^CHAPTER\\s+[A-Z]+\\s*$'), # CHAPTER X\n",
    "    ('MrsDalloway', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('NightAndDay', r'^CHAPTER\\s+[IVXLCDM]+\\s*$'),# CHAPTER ? (roman numeral)\n",
    "    ('Orlando', r'^CHAPTER\\s+\\d+\\.\\s*$'), # CHAPTER X. \n",
    "    ('TheVoyageOut', r'^Chapter\\s+[IVXLCDM]+\\s*$'), # Chapter ? (roman numeral)\n",
    "    ('TheWaves', r'^###CHAPTER###$'),  # annotation for 5 blank lines\n",
    "    ('TheYears', r'^\\s*(18|19)\\d{2}\\s*$'), # blank line, year, blank line\n",
    "    ('ToTheLighthouse', r'^\\s*\\d+\\s*$'), # blank line, number, blank line\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482dbbf-60c7-4d0e-8d18-17c143e57215",
   "metadata": {},
   "source": [
    "## Creating LIB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324e2dcd-0a44-4b9b-9695-a56386fdec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register each file to a library\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    book_id = source_file_path.split('/')[-1].replace('.utf8.txt', '')\n",
    "    book_title = source_file_path.split('/')[-1].replace('.utf8.txt', '')\n",
    "    book_data.append((book_id, source_file_path, book_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd82197-3d25-48ea-82bc-05a892244779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.DataFrame(book_data, columns=['book_id','source_file_path','title'])\\\n",
    "    .set_index('book_id').sort_index()\n",
    "\n",
    "# add chapter regexes\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "\n",
    "# add publication year\n",
    "publication_years = {\n",
    "    'TheVoyageOut': 1915,\n",
    "    'NightAndDay': 1919,\n",
    "    'JacobsRoom': 1922,\n",
    "    'MrsDalloway': 1925,\n",
    "    'ToTheLighthouse': 1927,\n",
    "    'Orlando': 1928,\n",
    "    'TheWaves': 1931,\n",
    "    'Flush': 1933,\n",
    "    'TheYears': 1937,\n",
    "    'BetweenTheActs': 1941\n",
    "}\n",
    "\n",
    "LIB['year'] = LIB['title'].map(publication_years)\n",
    "\n",
    "# add goodreads rating\n",
    "goodreads = {\n",
    "    'TheVoyageOut': 3.75,\n",
    "    'NightAndDay': 3.75,\n",
    "    'JacobsRoom': 3.69,\n",
    "    'MrsDalloway': 3.73,\n",
    "    'ToTheLighthouse': 3.78,\n",
    "    'Orlando': 3.86,\n",
    "    'TheWaves': 4.15,\n",
    "    'Flush': 3.87,\n",
    "    'TheYears': 3.77,\n",
    "    'BetweenTheActs': 3.61\n",
    "}\n",
    "\n",
    "LIB['goodreads'] = LIB['title'].map(goodreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e314df-934e-4e6d-9cb2-2a8390d81482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>year</th>\n",
       "      <th>goodreads</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BetweenTheActs</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>BetweenTheActs</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1941</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flush</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>Flush</td>\n",
       "      <td>^(CHAPTER\\s+[A-Z]+)\\s*$</td>\n",
       "      <td>1933</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JacobsRoom</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>JacobsRoom</td>\n",
       "      <td>^CHAPTER\\s+[A-Z]+\\s*$</td>\n",
       "      <td>1922</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrsDalloway</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>MrsDalloway</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1925</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightAndDay</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>NightAndDay</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCDM]+\\s*$</td>\n",
       "      <td>1919</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orlando</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>^CHAPTER\\s+\\d+\\.\\s*$</td>\n",
       "      <td>1928</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheVoyageOut</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheVoyageOut</td>\n",
       "      <td>^Chapter\\s+[IVXLCDM]+\\s*$</td>\n",
       "      <td>1915</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheWaves</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheWaves</td>\n",
       "      <td>^###CHAPTER###$</td>\n",
       "      <td>1931</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheYears</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>TheYears</td>\n",
       "      <td>^\\s*(18|19)\\d{2}\\s*$</td>\n",
       "      <td>1937</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ToTheLighthouse</th>\n",
       "      <td>/Users/lucyshichman/Documents/MSDS/DS5001/fina...</td>\n",
       "      <td>ToTheLighthouse</td>\n",
       "      <td>^\\s*\\d+\\s*$</td>\n",
       "      <td>1927</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source_file_path  \\\n",
       "book_id                                                              \n",
       "BetweenTheActs   /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "Flush            /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "JacobsRoom       /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "MrsDalloway      /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "NightAndDay      /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "Orlando          /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheVoyageOut     /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheWaves         /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "TheYears         /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "ToTheLighthouse  /Users/lucyshichman/Documents/MSDS/DS5001/fina...   \n",
       "\n",
       "                           title                 chap_regex  year  goodreads  \n",
       "book_id                                                                       \n",
       "BetweenTheActs    BetweenTheActs            ^###CHAPTER###$  1941       3.61  \n",
       "Flush                      Flush    ^(CHAPTER\\s+[A-Z]+)\\s*$  1933       3.87  \n",
       "JacobsRoom            JacobsRoom      ^CHAPTER\\s+[A-Z]+\\s*$  1922       3.69  \n",
       "MrsDalloway          MrsDalloway            ^###CHAPTER###$  1925       3.73  \n",
       "NightAndDay          NightAndDay  ^CHAPTER\\s+[IVXLCDM]+\\s*$  1919       3.75  \n",
       "Orlando                  Orlando       ^CHAPTER\\s+\\d+\\.\\s*$  1928       3.86  \n",
       "TheVoyageOut        TheVoyageOut  ^Chapter\\s+[IVXLCDM]+\\s*$  1915       3.75  \n",
       "TheWaves                TheWaves            ^###CHAPTER###$  1931       4.15  \n",
       "TheYears                TheYears       ^\\s*(18|19)\\d{2}\\s*$  1937       3.77  \n",
       "ToTheLighthouse  ToTheLighthouse                ^\\s*\\d+\\s*$  1927       3.78  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9dfb43-8e8c-48b3-993d-6ccc9b0e9300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482375.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate lengths of each document in characters\n",
    "length = LIB['source_file_path'].apply(lambda path: len(open(path, 'r', encoding='utf-8').read()))\n",
    "\n",
    "# find average length\n",
    "length.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e00b85-f37b-4608-84cf-0bb70f053eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "LIB.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/lib.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6602dd5-8795-42c4-b886-a17723540422",
   "metadata": {},
   "source": [
    "## Creating CORPUS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259424a3-4a57-4904-911f-d3288f3dea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter markers already applied in /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/BetweenTheActs.utf8.txt\n",
      "Chapter markers already applied in /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/MrsDalloway.utf8.txt\n",
      "Chapter markers already applied in /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheWaves.utf8.txt\n"
     ]
    }
   ],
   "source": [
    "# creating chapter markers for books with sections divided by multiple blank lines\n",
    "def insert_chapter_markers_exact(file_path):\n",
    "    # read in books\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # split text into two parts: before and after \"THE START\"\n",
    "    start_match = re.search(r'(?m)^THE START\\s*$', text)\n",
    "    if not start_match:\n",
    "        print(f\"Chapter markers already applied in {file_path}\")\n",
    "        return \"already applied\"\n",
    "    \n",
    "    start_idx = start_match.end()\n",
    "    header = text[:start_idx]\n",
    "    body = text[start_idx:]\n",
    "\n",
    "    # insert chapter marker immediately after \"THE START\"\n",
    "    body = re.sub(r'^(\\s*)', r'###CHAPTER###\\n\\1', body, count=1)\n",
    "\n",
    "    # replace exactly 5 blank lines with chapter marker\n",
    "    five_blank_pattern = r'(?m)(?:^[ \\t]*\\r?\\n){5}(?=^[^\\s])'\n",
    "    body = re.sub(five_blank_pattern, '\\n###CHAPTER###\\n', body)\n",
    "\n",
    "    # write back to file\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(header + body)\n",
    "\n",
    "    # confirm with print statement\n",
    "    print(f\"✅ Inserted chapter markers after 'THE START' and 5 blank lines in {file_path}\")\n",
    "\n",
    "\n",
    "# apply to the books that need it\n",
    "target_books = ['BetweenTheActs', 'MrsDalloway', 'TheWaves']\n",
    "for book_id in target_books:\n",
    "    file_path = LIB.loc[book_id].source_file_path\n",
    "    insert_chapter_markers_exact(file_path)\n",
    "    LIB.at[book_id, 'chap_regex'] = r'^###CHAPTER###$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7316674-016a-4fbc-b816-677c7fd691fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing function\n",
    "def tokenize_collection(LIB):\n",
    "    clip_pats = [\n",
    "    r\"(?m)^THE START\\s*$\",\n",
    "    r\"(?m)^THE END\\s*$\"\n",
    "    ]\n",
    "    \n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "        try:\n",
    "            print(f\"Tokenizing {book_id} {LIB.loc[book_id].title}\")\n",
    "            \n",
    "            chap_regex = LIB.loc[book_id].chap_regex\n",
    "            ohco_pats = [('chap', chap_regex, 'm')]\n",
    "            src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "            text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "            text.verbose = True\n",
    "            text.strip_hyphens = True\n",
    "            text.strip_whitespace = True\n",
    "\n",
    "            # debug: check if chapter regex is matching anything\n",
    "            with open(src_file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            matching_lines = pd.DataFrame({'line': [line.strip() for line in lines]})\n",
    "            num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
    "            print(f\"Found {num_matches} matching chapter headings for {book_id}\")\n",
    "\n",
    "            text.import_source().parse_tokens()\n",
    "            text.TOKENS['book_id'] = book_id\n",
    "            text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "            books.append(text.TOKENS)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Failed on {book_id}: {LIB.loc[book_id].title}\")\n",
    "            print(f\"Error: {e}\\n\")\n",
    "    \n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "    print(\"Done\")\n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "310bc25a-d607-426c-8afb-254424977bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing BetweenTheActs BetweenTheActs\n",
      "Found 36 matching chapter headings for BetweenTheActs\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/BetweenTheActs.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^###CHAPTER###$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing Flush Flush\n",
      "Found 6 matching chapter headings for Flush\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/Flush.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(CHAPTER\\s+[A-Z]+)\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/7jsc8p0d2kdg9phz71qccnd00000gn/T/ipykernel_4649/2936233021.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
      "/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis/textparser.py:97: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing JacobsRoom JacobsRoom\n",
      "Found 14 matching chapter headings for JacobsRoom\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/JacobsRoom.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[A-Z]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing MrsDalloway MrsDalloway\n",
      "Found 10 matching chapter headings for MrsDalloway\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/MrsDalloway.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^###CHAPTER###$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing NightAndDay NightAndDay\n",
      "Found 34 matching chapter headings for NightAndDay\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/NightAndDay.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[IVXLCDM]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing Orlando Orlando\n",
      "Found 6 matching chapter headings for Orlando\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/Orlando.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+\\d+\\.\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing TheVoyageOut TheVoyageOut\n",
      "Found 27 matching chapter headings for TheVoyageOut\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheVoyageOut.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^Chapter\\s+[IVXLCDM]+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing TheWaves TheWaves\n",
      "Found 29 matching chapter headings for TheWaves\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheWaves.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^###CHAPTER###$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Tokenizing TheYears TheYears\n",
      "Found 10 matching chapter headings for TheYears\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/TheYears.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*(18|19)\\d{2}\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/7jsc8p0d2kdg9phz71qccnd00000gn/T/ipykernel_4649/2936233021.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  num_matches = matching_lines[\"line\"].str.contains(chap_regex, regex=True).sum()\n",
      "/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/analysis/textparser.py:97: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ToTheLighthouse ToTheLighthouse\n",
      "Found 43 matching chapter headings for ToTheLighthouse\n",
      "Importing  /Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/woolf_novels/utf8/ToTheLighthouse.utf8.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK sentence tokenizer\n",
      "Parsing OHCO level 3 token_num by NLTK tokenization\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "531a70bd-5bc0-41b2-99d9-bb1b0f62ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BetweenTheActs</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(It, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>It</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(was, VBD)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(summer's, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>summer's</td>\n",
       "      <td>summers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(night, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>night</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ToTheLighthouse</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">43</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">9</th>\n",
       "      <th>10</th>\n",
       "      <td>(I, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(have, VBP)</td>\n",
       "      <td>VBP</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(had, VBN)</td>\n",
       "      <td>VBN</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(my, PRP$)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(vision., NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>vision.</td>\n",
       "      <td>vision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>865367 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          pos_tuple   pos  \\\n",
       "book_id         chap_id para_num sent_num token_num                         \n",
       "BetweenTheActs  1       0        0        0               (It, PRP)   PRP   \n",
       "                                          1              (was, VBD)   VBD   \n",
       "                                          2                 (a, DT)    DT   \n",
       "                                          3          (summer's, JJ)    JJ   \n",
       "                                          4             (night, NN)    NN   \n",
       "...                                                             ...   ...   \n",
       "ToTheLighthouse 43      3        9        10               (I, PRP)   PRP   \n",
       "                                          11            (have, VBP)   VBP   \n",
       "                                          12             (had, VBN)   VBN   \n",
       "                                          13             (my, PRP$)  PRP$   \n",
       "                                          14          (vision., NN)    NN   \n",
       "\n",
       "                                                    token_str term_str  \n",
       "book_id         chap_id para_num sent_num token_num                     \n",
       "BetweenTheActs  1       0        0        0                It       it  \n",
       "                                          1               was      was  \n",
       "                                          2                 a        a  \n",
       "                                          3          summer's  summers  \n",
       "                                          4             night    night  \n",
       "...                                                       ...      ...  \n",
       "ToTheLighthouse 43      3        9        10                I        i  \n",
       "                                          11             have     have  \n",
       "                                          12              had      had  \n",
       "                                          13               my       my  \n",
       "                                          14          vision.   vision  \n",
       "\n",
       "[865367 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f1debe-4f13-407d-aecb-0348f160d2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id\n",
       "BetweenTheActs      47059\n",
       "Flush               34610\n",
       "JacobsRoom          55494\n",
       "MrsDalloway         64389\n",
       "NightAndDay        168036\n",
       "Orlando             79225\n",
       "TheVoyageOut       137843\n",
       "TheWaves            78067\n",
       "TheYears           130731\n",
       "ToTheLighthouse     69913\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.groupby('book_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8efd9c5-cc7d-4234-b5e5-e535da708fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add POS group (per assignment specifications)\n",
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad16f8f-46d4-4d77-ac5c-df01d7044d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BetweenTheActs</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(It, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>It</td>\n",
       "      <td>it</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(was, VBD)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(summer's, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>summer's</td>\n",
       "      <td>summers</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(night, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>night</td>\n",
       "      <td>night</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         pos_tuple  pos  \\\n",
       "book_id        chap_id para_num sent_num token_num                        \n",
       "BetweenTheActs 1       0        0        0               (It, PRP)  PRP   \n",
       "                                         1              (was, VBD)  VBD   \n",
       "                                         2                 (a, DT)   DT   \n",
       "                                         3          (summer's, JJ)   JJ   \n",
       "                                         4             (night, NN)   NN   \n",
       "\n",
       "                                                   token_str term_str  \\\n",
       "book_id        chap_id para_num sent_num token_num                      \n",
       "BetweenTheActs 1       0        0        0                It       it   \n",
       "                                         1               was      was   \n",
       "                                         2                 a        a   \n",
       "                                         3          summer's  summers   \n",
       "                                         4             night    night   \n",
       "\n",
       "                                                   pos_group  \n",
       "book_id        chap_id para_num sent_num token_num            \n",
       "BetweenTheActs 1       0        0        0                PR  \n",
       "                                         1                VB  \n",
       "                                         2                DT  \n",
       "                                         3                JJ  \n",
       "                                         4                NN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc028f93-6cd5-4c68-bc5c-8ee2ac5b52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing anamolies\n",
    "CORPUS = CORPUS[CORPUS.term_str != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46736bf-da1a-4123-a9be-d8d24e25726f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BetweenTheActs</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(It, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>It</td>\n",
       "      <td>it</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(was, VBD)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(summer's, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>summer's</td>\n",
       "      <td>summers</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(night, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>night</td>\n",
       "      <td>night</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ToTheLighthouse</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">43</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">9</th>\n",
       "      <th>10</th>\n",
       "      <td>(I, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(have, VBP)</td>\n",
       "      <td>VBP</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(had, VBN)</td>\n",
       "      <td>VBN</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(my, PRP$)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(vision., NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>vision.</td>\n",
       "      <td>vision</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>864391 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          pos_tuple   pos  \\\n",
       "book_id         chap_id para_num sent_num token_num                         \n",
       "BetweenTheActs  1       0        0        0               (It, PRP)   PRP   \n",
       "                                          1              (was, VBD)   VBD   \n",
       "                                          2                 (a, DT)    DT   \n",
       "                                          3          (summer's, JJ)    JJ   \n",
       "                                          4             (night, NN)    NN   \n",
       "...                                                             ...   ...   \n",
       "ToTheLighthouse 43      3        9        10               (I, PRP)   PRP   \n",
       "                                          11            (have, VBP)   VBP   \n",
       "                                          12             (had, VBN)   VBN   \n",
       "                                          13             (my, PRP$)  PRP$   \n",
       "                                          14          (vision., NN)    NN   \n",
       "\n",
       "                                                    token_str term_str  \\\n",
       "book_id         chap_id para_num sent_num token_num                      \n",
       "BetweenTheActs  1       0        0        0                It       it   \n",
       "                                          1               was      was   \n",
       "                                          2                 a        a   \n",
       "                                          3          summer's  summers   \n",
       "                                          4             night    night   \n",
       "...                                                       ...      ...   \n",
       "ToTheLighthouse 43      3        9        10                I        i   \n",
       "                                          11             have     have   \n",
       "                                          12              had      had   \n",
       "                                          13               my       my   \n",
       "                                          14          vision.   vision   \n",
       "\n",
       "                                                    pos_group  \n",
       "book_id         chap_id para_num sent_num token_num            \n",
       "BetweenTheActs  1       0        0        0                PR  \n",
       "                                          1                VB  \n",
       "                                          2                DT  \n",
       "                                          3                JJ  \n",
       "                                          4                NN  \n",
       "...                                                       ...  \n",
       "ToTheLighthouse 43      3        9        10               PR  \n",
       "                                          11               VB  \n",
       "                                          12               VB  \n",
       "                                          13               PR  \n",
       "                                          14               NN  \n",
       "\n",
       "[864391 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e14908-003e-41b5-a495-984e3e6f89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "CORPUS.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/corpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb9087-1f75-4f5d-a7ae-6346e8e77d67",
   "metadata": {},
   "source": [
    "## Creating VOCAB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6f96f14-2817-4f70-b4c3-1fbde93d7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocab table\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "\n",
    "# length\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "\n",
    "# p and i\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "\n",
    "# max_pos and pax_pos_group\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['max_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13d294a7-f006-4bec-85f5-6ea2c092c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding porter stems\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer1 = PorterStemmer()\n",
    "VOCAB['stem_porter'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "# adding stopwords\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "\n",
    "VOCAB['stop'] = VOCAB.index.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76e3291f-4b53-4a42-859e-70e6276bb37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>driver</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>15.908146</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>driver</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snowing</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VB</td>\n",
       "      <td>snow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unapprehended</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>unapprehend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amuse</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>16.715501</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>amus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mumble</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>mumbl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnold</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>17.715501</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>arnold</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bawling</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>17.393573</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VB</td>\n",
       "      <td>bawl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisher</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>wisher</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massively</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>massiv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because</th>\n",
       "      <td>444</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>10.921085</td>\n",
       "      <td>IN</td>\n",
       "      <td>IN</td>\n",
       "      <td>becaus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 n  n_chars         p          i max_pos max_pos_group  \\\n",
       "term_str                                                                 \n",
       "driver          14        6  0.000016  15.908146      NN            NN   \n",
       "snowing          1        7  0.000001  19.715501     VBG            VB   \n",
       "unapprehended    1       13  0.000001  19.715501      JJ            JJ   \n",
       "amuse            8        5  0.000009  16.715501      VB            VB   \n",
       "mumble           1        6  0.000001  19.715501      JJ            JJ   \n",
       "arnold           4        6  0.000005  17.715501     NNP            NN   \n",
       "bawling          5        7  0.000006  17.393573     VBG            VB   \n",
       "wisher           1        6  0.000001  19.715501      NN            NN   \n",
       "massively        1        9  0.000001  19.715501      RB            RB   \n",
       "because        444        7  0.000516  10.921085      IN            IN   \n",
       "\n",
       "               stem_porter  stop  \n",
       "term_str                          \n",
       "driver              driver     0  \n",
       "snowing               snow     0  \n",
       "unapprehended  unapprehend     0  \n",
       "amuse                 amus     0  \n",
       "mumble               mumbl     0  \n",
       "arnold              arnold     0  \n",
       "bawling               bawl     0  \n",
       "wisher              wisher     0  \n",
       "massively           massiv     0  \n",
       "because             becaus     1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80379478-dc22-4a7f-b180-17a0bff796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "VOCAB.to_csv(\"/Users/lucyshichman/Documents/MSDS/DS5001/final_project/woolf2vec/output/vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3605450b-2484-41f7-bdfc-1f54ec792aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>17.715501</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.715501</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>10th</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwinglers</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.715501</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>zwingler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>à</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>éclair</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.715501</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>éclair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>éclairs</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>éclair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>êtres</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.715501</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NN</td>\n",
       "      <td>être</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24287 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           n  n_chars         p          i max_pos max_pos_group stem_porter  \\\n",
       "term_str                                                                       \n",
       "1          4        1  0.000005  17.715501      NN            NN           1   \n",
       "10         2        2  0.000002  18.715501      JJ            NN          10   \n",
       "1030       1        4  0.000001  19.715501      CD            CD        1030   \n",
       "10th       1        4  0.000001  19.715501      CD            CD        10th   \n",
       "112        1        3  0.000001  19.715501      CD            CD         112   \n",
       "...       ..      ...       ...        ...     ...           ...         ...   \n",
       "zwinglers  2        9  0.000002  18.715501     NNP            NN    zwingler   \n",
       "à          1        1  0.000001  19.715501      NN            NN           à   \n",
       "éclair     2        6  0.000002  18.715501      NN            NN      éclair   \n",
       "éclairs    1        7  0.000001  19.715501     NNP            NN      éclair   \n",
       "êtres      1        5  0.000001  19.715501     NNS            NN        être   \n",
       "\n",
       "           stop  \n",
       "term_str         \n",
       "1             0  \n",
       "10            0  \n",
       "1030          0  \n",
       "10th          0  \n",
       "112           0  \n",
       "...         ...  \n",
       "zwinglers     0  \n",
       "à             0  \n",
       "éclair        0  \n",
       "éclairs       0  \n",
       "êtres         0  \n",
       "\n",
       "[24287 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae953c7-7317-4566-82f2-30cee8de3e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
